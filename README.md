# Language Classifier

Для работы с данными, загрузки и обучения моделей используются **Transformers** и **Datasets**

## TO-DO
1. Улучшить качество кода и логгирование (PathLib, logger, рефакторинг кода)
2. Перенести код модели №3 из ноутбуков в .py файлы, добавить ее обучение и инференс (_не успел это сделать, увлекся опенсорсом моделей и доработкой кода_)

## Определение языка текста
Модели обучаются для определения **10 различных языков**: 

| Язык            | Сокращение |
|-----------------|------------|
| русский         | ru         |
| украинский      | uk         |
| белорусский     | be         |
| казахский       | kk         |
| азербайджанский | az         |
| армянский       | hy         |
| грузинский      | ka         |
| иврит           | he         |
| английский      | en         |
| немецкий        | de         |

### Типы задач (и мои итоговые open-sourced модели)
1. Базовая детекция языка одноязычных текстов (**multiclass**) - https://huggingface.co/nikitast/lang-classifier-roberta
2. Определение набора языков мультиязычных текстов (**multilabel**) - https://huggingface.co/nikitast/multilang-classifier-roberta
3. Определение участков конкретных языков в мультиязычных текстах (**token-based**) - https://huggingface.co/nikitast/lang-segmentation-roberta

### Данные
Так как для обучения моделей имеется только личный компьютер, а также Google Colaboratory и Kaggle, которые жестко контролируют время использования видеокарты и активность пользователя во время обучения, для обучения были выбраны данные небольших размеров

| Ресурс          | Языки          |
|-----------------|----------------|
| open_subtitles    | ka, he, en, de |
| oscar      | be, kk, az, hu |
| tatoeba     | ru, uk         |

Данные для русского и украинского языков сначала тоже брались из open_subtitles, но потом ресурс был изменен из-за плохого качества данных (в украинском датасете было ~10% русских реплик)

Итого по **100,000 семплов** для каждого языка, но для обучения брали только по **10,000 семплов** (из-за сильных ограничений по мощностям)

Задача однозначная, семплы практически всегда однозначно классифицируются - поэтому для обучения достойной модели достаточно несколько тысяч семплов для каждого языка

### Выбор архитектуры и обучение модели
За основу была взята модель **xlm-roberta-base** - https://huggingface.co/xlm-roberta-base
1. RoBERTa отлично зарекомендовала себя в задачах классификации текстов
2. Модель уже была предобучена на данных, содержащих 100 различных языков, и имеет представление о зависимостях и взаимодействиях различных языков и языковых групп друг с другом
3. Токенизатор модели был обучен в том числе на 10 языках, которые нам интересны

### Подробнее о задачах
1. **multiclass** - тут все очевидно - после построения multiclass датасета просто дообучаем модель на задачу **Sequence Classification**, каждому семплу соотносится 1 класс
2. **multilabel** - сначала создаем multilabel датасет, для этого каждый семпл трансформируем следующим образом - к исходному тексту прибавляем случайное количество случайно выбранных тексту с любыми языками, каждый текст предварительно обрезаем (тоже случайным образом). После этого дообучиваем исходную модель на задачу **Multi-Label Sequence Classification**
3. **token-based** - берем алгоритм создания multilabel датасета, но теперь назначаем язык для каждого слова (внутри происходит хитрый маппинг токенов и слов) - и обучаем модель на задачу **Token Classification**

### Использование репозитория

Установка зависимостей

`pip install --upgrade pip`

`pip install -r requirements.txt`

Загрузка и сохранение датасетов для дальнейшей обработки (для каждого языка)

`python3 lang_classifier/preprocessing/data_loader.py --save-to datasets`

После загрузки языковых датасетов создаем train и validation датасеты (в зависимости от параметров, single/multi-label)

`python3 lang_classifier/preprocessing/dataset_builder.py --save-to datasets --do-multilabel`

Запускаем скрипт обучения модели

`python3 lang_classifier/model/train.py --config-path configs/train_config.yaml`

Подаем в скрипт csv файл с текстами, получаем предсказания модели, путь к которой указали

`python3 lang_classifier/model/predict.py --model-path lang_model
`