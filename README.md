# Language Classifier

Для работы с данными, загрузки и обучения моделей используются Deep Learning библиотеки - Transformers и Datasets

###Модели для определения языка текста
Модели обучаются для определения 10 различных языков: 

| Язык            | Сокращение |
|-----------------|------------|
| русский         | ru         |
| украинский      | uk         |
| белорусский     | be         |
| казахский       | kk         |
| азербайджанский | az         |
| армянский       | hy         |
| грузинский      | ka         |
| иврит           | he         |
| английский      | en         |
| немецкий        | de         |

## Типы задач
1. Базовая детекция языка одноязычных текстов (**multiclass**)
2. Определение набора языков мультиязычных текстов (**multilabel**)
3. Определение участков конкретных языков в мультиязычных текстах (**token-based**)

## Данные
Так как для обучения моделей имеется только личный компьютер, а также Google Colaboratory и Kaggle, которые жестко контролируют время использования видеокарты и активность пользователя во время обучения, для обучения были выбраны данные небольших размеров

| Ресурс          | Языки          |
|-----------------|----------------|
| open_subtitles    | ka, he, en, de |
| oscar      | be, kk, az, hu |
| tatoeba     | ru, uk         |

Данные для русского и украинского языков сначала тоже брались из open_subtitles, но потом ресурс был изменен из-за плохого качества данных (в украинском датасете было ~10% русских реплик)

Итого по 100,000 семплов для каждого языка, но для обучения брали только 10,000 семплов (из-за сильных ограничений по мощностям)

Задача однозначная, семплы практически всегда однозначно классифицируются - поэтому для обучения достойной модели достаточно несколько тысяч семплов для каждого языка

## Выбор архитектуры и обучение модели
За основу была взята модель **xlm-roberta-base** - https://huggingface.co/xlm-roberta-base
1. RoBERTa отлично зарекомендовала себя в задачах классификации текстов
2. Модель уже была предобучена на данных, содержащих 100 различных языков, и имеет представление о зависимостях и взаимодействиях различных языков и языковых групп друг с другом
3. Токенизатор модели был обучен в том числе на 10 языках, которые нам интересны