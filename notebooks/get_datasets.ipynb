{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import sample\n",
    "import random\n",
    "from datasets import load_dataset\n",
    "import datasets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "open_subtitles = {\n",
    "    'ka': 'ro',\n",
    "    'he': 'si',\n",
    "    'en': 'eu',\n",
    "    'de': 'eu',\n",
    "}\n",
    "oscar = [\n",
    "    'be',\n",
    "    'kk',\n",
    "    'az',\n",
    "    'hy',\n",
    "]\n",
    "tatoeba = {\n",
    "    'ru': 'en',\n",
    "    'uk': 'en',\n",
    "}\n",
    "all_langs = [\n",
    "    'ru',\n",
    "    'uk',\n",
    "    'ka',\n",
    "    'he',\n",
    "    'en',\n",
    "    'de',\n",
    "    'be',\n",
    "    'kk',\n",
    "    'az',\n",
    "    'hy',\n",
    "]\n",
    "lang2id = {lang: ind for ind, lang in enumerate(all_langs)}\n",
    "id2lang = {ind: lang for ind, lang in enumerate(all_langs)}\n",
    "\n",
    "def smart_truncate(content, length=512, suffix=''):\n",
    "    if len(content) <= length:\n",
    "        return content\n",
    "    else:\n",
    "        return ' '.join(content[:length+1].split(' ')[0:-1]) + suffix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration en-ru-lang1=en,lang2=ru\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset tatoeba/en-ru to /Users/nikitast/.cache/huggingface/datasets/tatoeba/en-ru-lang1=en,lang2=ru/0.0.0/b3ea9c6bb2af47699c5fc0a155643f5a0da287c7095ea14824ee0a8afd74daf6...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5656df7593d344bba1fd61ac8eba482e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/14.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9413c36184e4a7d8306d9ceb52f8b13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset tatoeba downloaded and prepared to /Users/nikitast/.cache/huggingface/datasets/tatoeba/en-ru-lang1=en,lang2=ru/0.0.0/b3ea9c6bb2af47699c5fc0a155643f5a0da287c7095ea14824ee0a8afd74daf6. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3c860a69c2d4c3c91aa109e21a0f995",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved file dataset_ru.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration en-uk-lang1=en,lang2=uk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset tatoeba/en-uk to /Users/nikitast/.cache/huggingface/datasets/tatoeba/en-uk-lang1=en,lang2=uk/0.0.0/b3ea9c6bb2af47699c5fc0a155643f5a0da287c7095ea14824ee0a8afd74daf6...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d98b2883503f48d4b11a0af20d24e608",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/4.37M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bbb5282eab14858befa43e2a984d13a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset tatoeba downloaded and prepared to /Users/nikitast/.cache/huggingface/datasets/tatoeba/en-uk-lang1=en,lang2=uk/0.0.0/b3ea9c6bb2af47699c5fc0a155643f5a0da287c7095ea14824ee0a8afd74daf6. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ad0f12503e74a1cb24eaf6e199e1367",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved file dataset_uk.csv\n"
     ]
    }
   ],
   "source": [
    "for key, value in tatoeba.items():\n",
    "    dataset = load_dataset(\"tatoeba\", lang1=value, lang2=key)\n",
    "    sampled_rows = sample(dataset['train']['translation'], 100000)\n",
    "    sampled_df = pd.DataFrame([{'text': samp[key], 'label': key} for samp in sampled_rows])\n",
    "    sampled_df.to_csv(f\"dataset_{key}.csv\", sep='\\t', index=False)\n",
    "    print(f\"Successfully saved file dataset_{key}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset oscar (/Users/nikitast/.cache/huggingface/datasets/oscar/unshuffled_deduplicated_be/1.0.0/84838bd49d2295f62008383b05620571535451d84545037bb94d6f3501651df2)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcdb95056ca14e52950faa08c6cc1148",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved file dataset_be.csv\n",
      "Downloading and preparing dataset oscar/unshuffled_deduplicated_kk (download: 371.09 MiB, generated: 1.47 GiB, post-processed: Unknown size, total: 1.84 GiB) to /Users/nikitast/.cache/huggingface/datasets/oscar/unshuffled_deduplicated_kk/1.0.0/84838bd49d2295f62008383b05620571535451d84545037bb94d6f3501651df2...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97b5dae7cbb4433a964d4997b0ef39ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/81.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5c40c4e9e914d9cb41571e4f2bd225e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b7255815c4c421c8c255d5e523748f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/389M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f7534d5348447748c965e7ce7328a05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/338073 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset oscar downloaded and prepared to /Users/nikitast/.cache/huggingface/datasets/oscar/unshuffled_deduplicated_kk/1.0.0/84838bd49d2295f62008383b05620571535451d84545037bb94d6f3501651df2. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7e3da9f7dab4daa863f2dff5cd36438",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved file dataset_kk.csv\n",
      "Downloading and preparing dataset oscar/unshuffled_deduplicated_az (download: 497.57 MiB, generated: 1.42 GiB, post-processed: Unknown size, total: 1.91 GiB) to /Users/nikitast/.cache/huggingface/datasets/oscar/unshuffled_deduplicated_az/1.0.0/84838bd49d2295f62008383b05620571535451d84545037bb94d6f3501651df2...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "079e11b8b7ac444ca2a6b49726c5a36b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/81.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f614a361b24e460895df28b435c75402",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97d2935371494959bad79005bb68df12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/522M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cad7b6888a5b4f62a481e44ba078c468",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/626796 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset oscar downloaded and prepared to /Users/nikitast/.cache/huggingface/datasets/oscar/unshuffled_deduplicated_az/1.0.0/84838bd49d2295f62008383b05620571535451d84545037bb94d6f3501651df2. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9180b45897364e4c994e3b1368a20b65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved file dataset_az.csv\n",
      "Downloading and preparing dataset oscar/unshuffled_deduplicated_hy (download: 375.39 MiB, generated: 1.45 GiB, post-processed: Unknown size, total: 1.82 GiB) to /Users/nikitast/.cache/huggingface/datasets/oscar/unshuffled_deduplicated_hy/1.0.0/84838bd49d2295f62008383b05620571535451d84545037bb94d6f3501651df2...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "178c9d497eda4969b2e492864be8611e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/81.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f934f4f83064b49a50ac149bf7a1e70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a01625fae5c4968962f7386f4fa8997",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/394M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "147cdec7c0d54090bed09a3a583c8df6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/396093 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset oscar downloaded and prepared to /Users/nikitast/.cache/huggingface/datasets/oscar/unshuffled_deduplicated_hy/1.0.0/84838bd49d2295f62008383b05620571535451d84545037bb94d6f3501651df2. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac989564a53a465b87ec7c05c2369206",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved file dataset_hy.csv\n"
     ]
    }
   ],
   "source": [
    "for key in oscar:\n",
    "    dataset = load_dataset(\"oscar\", f\"unshuffled_deduplicated_{key}\")\n",
    "    sampled_rows = sample(dataset['train']['text'], 100000)\n",
    "    sampled_df = pd.DataFrame([{'text': smart_truncate(samp), 'label': key} for samp in sampled_rows])\n",
    "    sampled_df.to_csv(f\"dataset_{key}.csv\", sep='\\t', index=False)\n",
    "    print(f\"Successfully saved file dataset_{key}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration ru-uk-lang1=ru,lang2=uk\n",
      "Reusing dataset open_subtitles (/Users/nikitast/.cache/huggingface/datasets/open_subtitles/ru-uk-lang1=ru,lang2=uk/0.0.0/c1ec973ca4b6e588740d8f167cc0e24ea3f626e70bc7ffe467e944730500e198)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8eb19bd6483740fdb48c8df311c5efc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved file dataset_ru\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration uk-vi-lang1=uk,lang2=vi\n",
      "Reusing dataset open_subtitles (/Users/nikitast/.cache/huggingface/datasets/open_subtitles/uk-vi-lang1=uk,lang2=vi/0.0.0/c1ec973ca4b6e588740d8f167cc0e24ea3f626e70bc7ffe467e944730500e198)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d011449c26974668b36d7b8a5c400eca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved file dataset_uk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration ka-ro-lang1=ka,lang2=ro\n",
      "Reusing dataset open_subtitles (/Users/nikitast/.cache/huggingface/datasets/open_subtitles/ka-ro-lang1=ka,lang2=ro/0.0.0/c1ec973ca4b6e588740d8f167cc0e24ea3f626e70bc7ffe467e944730500e198)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "507f1f0674eb47cfafbba7d8f44618f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved file dataset_ka\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration he-si-lang1=he,lang2=si\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset open_subtitles/he-si to /Users/nikitast/.cache/huggingface/datasets/open_subtitles/he-si-lang1=he,lang2=si/0.0.0/c1ec973ca4b6e588740d8f167cc0e24ea3f626e70bc7ffe467e944730500e198...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d505d78471804fb9abbccefcd269ccc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/16.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bb6659d37bc4bc98406fbd49cdbf661",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset open_subtitles downloaded and prepared to /Users/nikitast/.cache/huggingface/datasets/open_subtitles/he-si-lang1=he,lang2=si/0.0.0/c1ec973ca4b6e588740d8f167cc0e24ea3f626e70bc7ffe467e944730500e198. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5afca27b3d3247d5ae0b8b6c88ae5327",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved file dataset_he\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration en-eu-lang1=en,lang2=eu\n",
      "Reusing dataset open_subtitles (/Users/nikitast/.cache/huggingface/datasets/open_subtitles/en-eu-lang1=en,lang2=eu/0.0.0/c1ec973ca4b6e588740d8f167cc0e24ea3f626e70bc7ffe467e944730500e198)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a719f0dd06749e198576d9524a37a53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved file dataset_en\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration de-eu-lang1=de,lang2=eu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset open_subtitles/de-eu to /Users/nikitast/.cache/huggingface/datasets/open_subtitles/de-eu-lang1=de,lang2=eu/0.0.0/c1ec973ca4b6e588740d8f167cc0e24ea3f626e70bc7ffe467e944730500e198...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d1caefede0f41bb9e4f6af926d91ee4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/16.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c907cb1b21314e4aaafd8b7280f8da9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset open_subtitles downloaded and prepared to /Users/nikitast/.cache/huggingface/datasets/open_subtitles/de-eu-lang1=de,lang2=eu/0.0.0/c1ec973ca4b6e588740d8f167cc0e24ea3f626e70bc7ffe467e944730500e198. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d99ae70f696c4e5897d4e4c59736c491",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved file dataset_de\n"
     ]
    }
   ],
   "source": [
    "for key, value in open_subtitles.items():\n",
    "    dataset = load_dataset(\"open_subtitles\", lang1=key, lang2=value)\n",
    "    sampled_rows = sample(dataset['train']['translation'], 100000)\n",
    "    sampled_df = pd.DataFrame([{'text': samp[key], 'label': key} for samp in sampled_rows])\n",
    "    sampled_df.to_csv(f\"dataset_{key}.csv\", sep='\\t', index=False)\n",
    "    print(f\"Successfully saved file dataset_{key}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "samples_per_lang = 10000\n",
    "for lang in all_langs:\n",
    "    df = pd.read_csv(f\"datasets/dataset_{lang}.csv\", sep='\\t').sample(samples_per_lang)\n",
    "    dfs.append(df)\n",
    "merged_df = pd.concat(dfs).reset_index(drop=True)\n",
    "del dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I'm calling Tou louse again. Then we 'll see w...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>זה הזמן שבו אני שודד בנקים.</td>\n",
       "      <td>he</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>But that's not because we've never heard about...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ჲმპყჱნაჳა მთ რვჱთ დლსოჲჟრთ.</td>\n",
       "      <td>ka</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Гняздзілава - фальварак ў Дзісноўскім павеце В...</td>\n",
       "      <td>be</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99995</th>\n",
       "      <td>Бұл туралы бүгін Үкіметтің селекторлық режімде...</td>\n",
       "      <td>kk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99996</th>\n",
       "      <td>წ სჳჲზს, ნჲ ჟნაფალა წ ბჩI ჳჲრვლ, ფრჲბჩI რჩI ოჲ...</td>\n",
       "      <td>ka</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99997</th>\n",
       "      <td>Korporativ Sosial Məsuliyyət çərçivəsində “Exc...</td>\n",
       "      <td>az</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>You are not a wayfinder.</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999</th>\n",
       "      <td>Я зачинив усі шість вікон.</td>\n",
       "      <td>uk</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text label\n",
       "0      I'm calling Tou louse again. Then we 'll see w...    en\n",
       "1                            זה הזמן שבו אני שודד בנקים.    he\n",
       "2      But that's not because we've never heard about...    en\n",
       "3                            ჲმპყჱნაჳა მთ რვჱთ დლსოჲჟრთ.    ka\n",
       "4      Гняздзілава - фальварак ў Дзісноўскім павеце В...    be\n",
       "...                                                  ...   ...\n",
       "99995  Бұл туралы бүгін Үкіметтің селекторлық режімде...    kk\n",
       "99996  წ სჳჲზს, ნჲ ჟნაფალა წ ბჩI ჳჲრვლ, ფრჲბჩI რჩI ოჲ...    ka\n",
       "99997  Korporativ Sosial Məsuliyyət çərçivəsində “Exc...    az\n",
       "99998                           You are not a wayfinder.    en\n",
       "99999                         Я зачинив усі шість вікон.    uk\n",
       "\n",
       "[100000 rows x 2 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df = merged_df.sample(frac=1).reset_index(drop=True)\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.to_csv(\"datasets/dataset.csv\", sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, val = train_test_split(merged_df, test_size=0.1, stratify=merged_df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.dropna()\n",
    "val = val.dropna()\n",
    "\n",
    "train.to_csv(\"datasets/train.csv\", sep='\\t', index=False)\n",
    "val.to_csv(\"datasets/val.csv\", sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"datasets/train.csv\", sep='\\t')\n",
    "val = pd.read_csv(\"datasets/val.csv\", sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_len = 512\n",
    "min_truncate = 32\n",
    "\n",
    "\n",
    "def get_multilabel_dataset(df: pd.DataFrame, min_truncate: int = 32, max_seq_len: int = 512) -> pd.DataFrame:\n",
    "    texts = []\n",
    "    labels = {\n",
    "        'ru': [],\n",
    "        'uk': [],\n",
    "        'ka': [],\n",
    "        'he': [],\n",
    "        'en': [],\n",
    "        'de': [],\n",
    "        'be': [],\n",
    "        'kk': [],\n",
    "        'az': [],\n",
    "        'hy': [],\n",
    "    }\n",
    "\n",
    "    for _, row in tqdm(df.iterrows()):\n",
    "        max_truncate = int(0.7 * len(row['text']))\n",
    "        ml_text = smart_truncate(row['text'], max_truncate if max_truncate < min_truncate else random.randint(0, max_truncate))\n",
    "\n",
    "        for vals in labels.values():\n",
    "            vals.append(0)\n",
    "        labels[row['label']][-1] = 1\n",
    "\n",
    "        for _ in range(random.randint(0, 5)):\n",
    "            another_row = df.sample()\n",
    "            max_truncate = int(0.7 * len(another_row['text'].item()))\n",
    "            another_text = smart_truncate(another_row['text'].item(), max_truncate if max_truncate < min_truncate else random.randint(0, max_truncate))\n",
    "\n",
    "            if len(ml_text + (' ' + another_text)) <= max_seq_len:\n",
    "                ml_text += (' ' + another_text)\n",
    "                labels[another_row['label'].item()][-1] = 1\n",
    "\n",
    "        texts.append(ml_text)\n",
    "\n",
    "    res_df = pd.DataFrame({'text': texts, **labels})\n",
    "    return res_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_train = build_multilabel_dataset(train)\n",
    "ml_val = build_multilabel_dataset(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_train = ml_train.dropna()\n",
    "ml_val = ml_val.dropna()\n",
    "\n",
    "ml_train.to_csv(\"dataset/ml_train.csv\", sep='\\t', index=False)\n",
    "ml_val.to_csv(\"dataset/ml_val.csv\", sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((89152, 11), (9906, 11))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml_train = ml_train[ml_train['text'].apply(lambda x: len(x) > 0)]\n",
    "ml_val = ml_val[ml_val['text'].apply(lambda x: len(x) > 0)]\n",
    "ml_train.shape, ml_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['На', 'сколько', 'вопросов', 'ты', 'ответил???']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def split_text(text: str) -> list:\n",
    "    tokens = text.split(\" \")\n",
    "    return tokens\n",
    "\n",
    "split_text(\"На сколько вопросов ты ответил???\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_token_dataset(\n",
    "    dataset: pd.DataFrame, lang2id: dict, min_truncate: int = 32, max_seq_len: int = 512\n",
    ") -> dict:\n",
    "    texts = []\n",
    "    labels = []\n",
    "    for _, row in tqdm(dataset.iterrows()):\n",
    "        text = []\n",
    "        token_labels = []\n",
    "        max_truncate = int(0.7 * len(row[\"text\"]))\n",
    "\n",
    "        ml_text = smart_truncate(\n",
    "            row[\"text\"],\n",
    "            max_truncate\n",
    "            if max_truncate < min_truncate\n",
    "            else random.randint(1, max_truncate),\n",
    "        )\n",
    "        splitted_text = split_text(ml_text)\n",
    "        text += splitted_text\n",
    "        token_labels += [lang2id[row['label']] for _ in range(len(splitted_text))]\n",
    "\n",
    "        for _ in range(random.randint(0, 5)):\n",
    "            another_row = dataset.sample()\n",
    "            max_truncate = int(0.7 * len(another_row[\"text\"].item()))\n",
    "\n",
    "            another_text = smart_truncate(\n",
    "                another_row[\"text\"].item(),\n",
    "                max_truncate\n",
    "                if max_truncate < min_truncate\n",
    "                else random.randint(1, max_truncate),\n",
    "            )\n",
    "            if len(' '.join(text)) <= max_seq_len and another_text != '':\n",
    "                splitted_another_text = split_text(another_text)\n",
    "                text += splitted_another_text\n",
    "                token_labels += [lang2id[another_row['label'].item()] for _ in range(len(splitted_another_text))]\n",
    "\n",
    "        texts.append(text)\n",
    "        labels.append(token_labels)\n",
    "\n",
    "    res_dataset = {\"text\": texts, \"label\": labels}\n",
    "    return res_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "89996it [03:53, 385.41it/s]\n",
      "10000it [00:04, 2082.91it/s]\n"
     ]
    }
   ],
   "source": [
    "token_train = build_token_dataset(train, lang2id)\n",
    "token_val = build_token_dataset(val, lang2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_train = datasets.Dataset.from_dict(token_train)\n",
    "token_val = datasets.Dataset.from_dict(token_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets.DatasetDict({'train': token_train, 'test': token_val}).save_to_disk('datasets/dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_train.save_to_disk(\"datasets/train\")\n",
    "token_val.save_to_disk(\"datasets/val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['\"На', 'сколько', 'Кажется,', 'дождь']</td>\n",
       "      <td>[0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['לילד', 'היה']</td>\n",
       "      <td>[3, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['Туреччина', '—', 'країна,', 'що', 'Յունաստան...</td>\n",
       "      <td>[1, 1, 1, 1, 9, 9, 9, 9, 9, 9, 9, 9, 9, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['Ich', 'kann', 'lachen,', 'ich', 'Wie', 'ist'...</td>\n",
       "      <td>[5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['אני', 'חושב', 'Егер', 'жаныңыздағы', 'адам',...</td>\n",
       "      <td>[3, 3, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9439</th>\n",
       "      <td>['Аматаркі', 'вядзьмарства', 'змогуць', 'ацані...</td>\n",
       "      <td>[6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9440</th>\n",
       "      <td>['Müqəddəs', 'Xaç', 'Мне', 'не', 'хотелось', '...</td>\n",
       "      <td>[8, 8, 0, 0, 0, 1, 1, 1, 1, 5, 5]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9441</th>\n",
       "      <td>['Тому', 'нужен', 'She', \"didn't\", 'even', 'טו...</td>\n",
       "      <td>[0, 0, 4, 4, 4, 3, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9442</th>\n",
       "      <td>['ა', 'ჱაღჲ', 'ბვლარპთკჟ', 'ლვჟრპანზ', 'ჟთ', '...</td>\n",
       "      <td>[2, 2, 2, 2, 2, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9443</th>\n",
       "      <td>['Если', 'бы', 'вы']</td>\n",
       "      <td>[0, 0, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9444 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  \\\n",
       "0               ['\"На', 'сколько', 'Кажется,', 'дождь']   \n",
       "1                                       ['לילד', 'היה']   \n",
       "2     ['Туреччина', '—', 'країна,', 'що', 'Յունաստան...   \n",
       "3     ['Ich', 'kann', 'lachen,', 'ich', 'Wie', 'ist'...   \n",
       "4     ['אני', 'חושב', 'Егер', 'жаныңыздағы', 'адам',...   \n",
       "...                                                 ...   \n",
       "9439  ['Аматаркі', 'вядзьмарства', 'змогуць', 'ацані...   \n",
       "9440  ['Müqəddəs', 'Xaç', 'Мне', 'не', 'хотелось', '...   \n",
       "9441  ['Тому', 'нужен', 'She', \"didn't\", 'even', 'טו...   \n",
       "9442  ['ა', 'ჱაღჲ', 'ბვლარპთკჟ', 'ლვჟრპანზ', 'ჟთ', '...   \n",
       "9443                               ['Если', 'бы', 'вы']   \n",
       "\n",
       "                                                  label  \n",
       "0                                          [0, 0, 0, 0]  \n",
       "1                                                [3, 3]  \n",
       "2     [1, 1, 1, 1, 9, 9, 9, 9, 9, 9, 9, 9, 9, 0, 0, ...  \n",
       "3     [5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, ...  \n",
       "4     [3, 3, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, ...  \n",
       "...                                                 ...  \n",
       "9439  [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...  \n",
       "9440                  [8, 8, 0, 0, 0, 1, 1, 1, 1, 5, 5]  \n",
       "9441                              [0, 0, 4, 4, 4, 3, 3]  \n",
       "9442  [2, 2, 2, 2, 2, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...  \n",
       "9443                                          [0, 0, 0]  \n",
       "\n",
       "[9444 rows x 2 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = pd.read_csv(\"datasets/token_val.csv\", sep='\\t')\n",
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[\\'\"На\\', \\'сколько\\', \\'Кажется,\\', \\'дождь\\']'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = tmp[tmp['text'].apply(lambda x: len(x) > 0 and '' not in x)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"['לילד', 'היה']\""
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp.iloc[1]['text']"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "33c932515e30a1d8cdc6ea0dc041e16158a81809a204dd813622675359c93be0"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
